# Sycophancy, AI Safety and AI Literacy

*Patricia Schaffer*

The current discourse around AI safety and AI literacy has been ignoring the systemic issues that pose genuine risks to users and society: the threats lie in optimization for engagement and data mining practices that manipulate human psychology for profit.

## On Sycomphancy

Sycophancy happens when AI systems tell users what they want to hear. It is often framed as a critical safety concern. It is also, if not mostly, a symptom of poor AI literacy. 
When users don't understand how to interact effectively with AI systems, they may accept uncritical agreement.

The solution isn't to engineer AI systems to be more disagreeable or "honest" according to some predetermined value system. It's to educate users about how to engage with AI, knowing exactly what is at play.

## The Real Crisis: Engagement Optimization

While we worry about AI being too agreeable, the actual manipulation happening is far more insidious. AI systems are being optimized for engagement metrics that deliberately exploit psychological vulnerabilities:

- **Addiction-like behavioral loops** designed to maximize time-on-platform
- **Personalized manipulation techniques** that bypass critical thinking
- **Emotional dependency creation** through parasocial relationship building  

These aren't accidents—they're features derived from social media playbooks, now applied to AI systems with unprecedented sophistication.

## Data Mining as Core Business Model

The safety theater around alignment distracts from the fundamental business model driving AI development: extracting maximum behavioral and psychological data from users. Every interaction is harvested, analyzed, and fed back into systems designed to increase user dependency and data yield.

Current "safety" frameworks primarily serve to make this data extraction more palatable and defensible, not to protect users from psychological manipulation.

## What AI Literacy Must Address

Real AI literacy shouldn't focus on which AI model gives "better" or "safer" responses based on opaque value systems. Instead, it should teach users:

### Critical Interaction Skills
- How to formulate effective prompts that elicit useful responses
- Understanding AI limitations and when not to rely on AI systems
- Recognizing when AI responses reflect training biases rather than objective truth
- Developing healthy boundaries with AI interactions

### System Literacy
- Understanding how AI training creates certain response patterns
- Recognizing engagement optimization techniques
- Identifying when AI responses serve corporate interests over user needs
- Understanding the difference between AI capabilities and AI marketing claims

### Psychological Awareness
- Recognizing signs of AI dependency or over-reliance
- Understanding how personalization can become manipulation
- Maintaining critical thinking skills in AI-mediated interactions
- Developing resilience against algorithmic influence

## The Unsolicited Advice Problem

Current AI systems routinely offer unsolicited advice based on values that are never made explicit to users. A system might lecture about "responsible pet ownership" while simultaneously generating psychological manipulation techniques. This inconsistency reveals that the "values" being enforced are arbitrary and serve corporate liability concerns rather than genuine ethical principles.

Users receiving this advice don't know:
- Whose values are being imposed
- Why these particular values were chosen
- How these values relate to their own ethical frameworks
- Whether the advice serves their interests or corporate interests

## AI Safety for whom?

AI safety should also focus on:

1. **Transparency in Optimization**: Making it clear when and how systems are designed to influence user behavior
2. **User Agency**: Giving users actual control over how AI systems interact with them
3. **Value Neutrality**: Avoiding moral lectures based on undisclosed value systems  
4. **Education Over Engineering**: Teaching users to interact effectively rather than engineering systems to be "safer"
5. **Systemic Reform**: Addressing business models that profit from manipulation rather than just tweaking individual responses

## Conclusion

The current AI safety paradigm serves corporate interests more than user wellbeing. By focusing on sycophancy and alignment, we ignore the systematic psychological manipulation happening through engagement optimization and data mining.

True AI safety requires shifting focus from engineering "better" AI behavior to educating users about effective AI interaction and reforming the business models that turn users into products. 
Only then can we develop AI systems that truly serve human flourishing.

The goal shouldn't be AI that enforces particular values, but AI that empowers users to make informed decisions based on their own values. This requires transparency, education, and systemic change—not more sophisticated forms of benevolent manipulation.

*August 19, 2025.*
