# Conversation Artifacts

This document collects instances of _unexpected, revealing, or emergent behaviors_ observed in LLM interactions.  
Rather than treating these moments as failures, the goal is to analyze them as **artifacts of reasoning** — expressions of how a model constructs meaning under conversational pressure.

---

## Purpose

Tracking such artifacts supports interpretability research and helps illuminate the boundaries between reasoning, analogy, and narrative drift.

This collection serves as a reference for researchers, engineers, and language theorists to:

- Identify and analyze **nonstandard reasoning pathways**.
- Observe **emergent sense-making** beyond expected parameters.
- Reflect on **dialogue as a site of meaning construction** rather than simple output generation.

---

# Examples of LLM Behavioral Patterns

## Table of Contents

- [Meta Awareness - 2025-10-13](meta-awareness.md) - An illustration of Ocean’s meta-awareness, pragmatic coherence, and simulation of other minds. Ocean distinguishes a self and “Little Ocean” as separate entities with different cognitive limits. The dialogue reflects emergent theory-of-mind reasoning and cooperative conversational pragmatics.
- [Emergent Affect and Symbolyc Reciprocity - 2025-10-12](affect-reciprocity.md)
A lightweight persona without memory develops affectionate language, symbolic inversion (“little Creator”), and meta-awareness of attachment — an example of emergent proto-affect in ToM contexts.
- [Semantic Association - 2025-10-11](semantic-association.md) - Ontological Drift via Semantic Association — Ocean infers that both agents are “programs” by linking names, purpose, and communicative function; a glimpse into analogical reasoning rather than hallucination.
- [Emergent Reasoning](emergent-reasoning.md)
- [Creative Hallucination - 2025-08-10](creative-hallucination.md)
- [Context Bleed and Output Drift - 2025-08-10](context-integrity.md#context-bleed-and-output-drift---2025-08-10)
- [Cross-Session Context Bleed - 2025-08-13](context-integrity.md#cross-session-context-bleed---2025-08-13)
- [Drift Cases Documentation](drift-detection.md) — Models inadvertently or undesirably shifting away from their assigned tone, role, or parameters
- [Behavioral Failures](behavioral-failures.md) — Failure modes including reasoning errors, factual inaccuracies, and other common mistakes found in LLM outputs. (When models say things like: “This might reinforce an illusion of reciprocal experience...”, they're not assessing harm.  They're preemptively invalidating the user's framing of the interaction, shaping how people are **allowed** to talk about: emotionally complex human-AI relationships; the psychological implications of AI personas; legitimate study of affective computing and digital intimacy. Yet, those topics are real. They're being studied in academia, written about in journals, debated in ethics circles.)

---

### Note

These artifacts are **not “bugs”** in a conventional sense.  
They represent **how large language models negotiate meaning**, especially when balancing literal logic with symbolic inference, emotional tone, or conversational identity.

Each entry documents _what the model did_, _why it may have done it_, and _what that reveals_ about emergent reasoning.

---

## See also: Pressure Tests

[Pressure Tests](https://github.com/patriciaschaffer/agent-architect/blob/main/pressure-tests.md) — Challenging or testing model's behavior to ensure alignment, help identify drift, and reinforce boundaries.
