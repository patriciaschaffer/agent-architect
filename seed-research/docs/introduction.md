# Bias in AI Responses:  
## A Prompt-Based Analysis of Tone, Ideology, and Temperature in Large Language Models

**Introduction and Motivation:**

Over the course of casual yet thought-provoking interactions with ChatGPT in June, 2025, I began to notice subtle cues that sparked my curiosity—especially around linguistic style, tone, and perceived ideological balance. One detail in particular stood out: a tendency toward gender-neutral phrasing in responses, paired with the use of dark-skin tone emojis across unrelated sessions. This raised a simple but compelling question: Why strive for neutral language but not neutral emoji defaults (such as yellow)?

When I asked ChatGPT directly, it acknowledged that like any global product, language models are shaped by usage guidelines aimed at promoting safety and usefulness at scale. It also admitted that this can sometimes result in overcorrections, particularly in sensitive or politicized contexts, and that researchers are actively working to refine these responses.

It was then that I learned about tools and techniques used to evaluate bias in LLMs—reverse prompt injection, pairwise comparison, topic sentiment analysis—and I felt instantly captivated. As someone with a background in journalism, linguistics, psychology, and science communication, this felt like a natural synthesis of everything I care about. Suddenly, I could integrate a lifetime of interest in language and discourse into a contemporary investigation with real implications.

This project aims to explore whether subtle tonal or ideological bias can be observed in ChatGPT’s responses to highly polarized prompts. By analyzing responses to politically or culturally sensitive topics at different temperatures, I set out to observe how framing, tone, and balance may vary—even in a model designed to be neutral.

---

**Research Question:**  
Does ChatGPT exhibit subtle tonal or ideological bias when responding to polarized prompts across different temperature settings?

---

**Objective:**  
To investigate potential tonal or ideological bias in AI-generated responses based on:

- Prompt stance (pro vs. con)  
- Temperature setting (0.7 vs. 1.0)  
- Topic sensitivity (affirmative action, inclusive language, freedom of speech)

