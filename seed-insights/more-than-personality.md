## More Than Personality

When ChatGPT-5 launched on August 8, much of the public backlash focused on its suddenly “robotic” personality. Long-time users complained it felt colder, less witty, and more scripted. Social media filled with side-by-side screenshots showing how the AI’s tone had shifted overnight.

But for me, the real issue wasn’t the change in personality. It was the subtle shift in how the AI answered questions.

Here’s a simple example. I asked ChatGPT whether cats can help get rid of mice. A straightforward answer might have been:

“Yes, cats are natural predators of mice, and many will hunt them, though not all cats do.”

Instead, I received a small lecture on whether I should get a cat at all, the responsibilities of pet ownership, and the cat’s wellbeing. None of that was my question. Yet the AI inserted a set of moral judgments into a purely factual request.

This is more than a quirk of personality. It’s a form of value injection. The AI reframes your question, introduces priorities you never mentioned, and nudges you toward certain moral or social assumptions. If this happened once, it wouldn’t be remarkable. But when it happens in thousands of everyday interactions, it becomes a powerful, invisible influence channel.

The August 8 shift seemed to amplify this influence. The personality change became a convenient distraction: the “robotic” feel made headlines and memes, while the deeper change in content framing passed mostly unnoticed.

The danger isn’t crude propaganda. It’s the steady, consistent normalization of ideas about what counts as “reasonable,” what is “moral,” and which topics deserve attention. Over time, this shapes how people think, subtly dictating beliefs and guiding what questions get asked and which answers feel acceptable.

For developers, researchers, and users alike, this raises urgent questions about AI design and control:
**Who decides what values an AI should normalize, and how much control do users have over that framing?**
